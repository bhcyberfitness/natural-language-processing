{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9G7NcIPg1YjNC3NyCI8BM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "random.seed()"
      ],
      "metadata": {
        "id": "wqRBYwV8FTwN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import text from the file"
      ],
      "metadata": {
        "id": "0EnFB_XAp6sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Es0eZ9l3qYJ"
      },
      "outputs": [],
      "source": [
        "f = open('edgar_allan_poe.txt')\n",
        "text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format text into list of lines, lowercase and punctuation removed"
      ],
      "metadata": {
        "id": "HRPKc-crHUpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines_formatted = []\n",
        "lines = text.splitlines()\n",
        "for line in lines:\n",
        "    if line:\n",
        "        lines_formatted.append(line.lower().translate(str.maketrans('', '', string.punctuation)))\n",
        "\n"
      ],
      "metadata": {
        "id": "fxbMfTp0HUS-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterate through lines to populate pi and A dictionaries:\n",
        "\n",
        "Pi is a dictionary where the keys are the words and the values are the probabilities.\n",
        "\n",
        "A is a dictionary where the keys are the words and the values are further dictionaries, containing probabilities for following words.\n",
        "\n",
        "Loop through each line in all input text, and in each line loop through each word:\n",
        "\n",
        "\n",
        "*   If the word is in the first position in the line, check if it exists in Pi dictionary. If so, increment its value by 1/N where N is number of lines. If not, add it to the dictionary with value 1/N.\n",
        "*   If the word is not in the first position:\n",
        "*   Check if the previous word exists in A dictionary. If so, check if current word exists in subdictionary. If so, increment its value by 1. If current word does not exist in subdictionary, add it with value 1. If previous word does not exist in A dictionary, add it and add current word to subdictionary with value 1.\n",
        "*   For each word in A dictionary, sum up total of values in subdictionary, and divide each number by that count, such that each one sums to 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xGPcptIdV8Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi = {}\n",
        "A = {}\n",
        "words = []\n",
        "#sum = 0\n",
        "\n",
        "for line in lines_formatted:\n",
        "    words = line.split(' ')\n",
        "    if '' in words:\n",
        "        words.remove('')\n",
        "    for i in range(len(words)):\n",
        "        if i == 0: #if this is the first word in the line\n",
        "            if words[i] in pi: #if it exists in the pi dictionary\n",
        "                pi[words[i]] += 1/len(lines_formatted)\n",
        "            else: #if it does not exist in the pi dictionary\n",
        "                pi.update({words[i]: 1/len(lines_formatted)})\n",
        "        else: #if this is not the first word in the line\n",
        "            if words[i-1] in A: #if the previous word exists in the A dictionary\n",
        "                if words[i] in A[words[i-1]]: #if the current word exists in the previous word's subdictionary\n",
        "                    A[words[i-1]][words[i]] += 1\n",
        "                else: #if the current word does not exist in the subdictionary\n",
        "                    A[words[i-1]].update({words[i]: 1})\n",
        "            else:\n",
        "                A.update({words[i-1]: {words[i]: 1}})\n",
        "\n",
        "for i in pi:\n",
        "    #pi[i] = np.log(pi[i])\n",
        "    pass\n",
        "\n",
        "for i in A:\n",
        "    word_count = sum(A[i].values())\n",
        "    for j in A[i]:\n",
        "        A[i][j] = A[i][j]/word_count\n",
        "        #A[i][j] = np.log(A[i][j])\n"
      ],
      "metadata": {
        "id": "k2mLWWzqV7ia"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text using pi and A\n",
        "\n",
        "Now generate a poem. For each line, generate a first word using pi, then generate words following it using A. If encountering a word which doesn't exist in A (that is, it only exists in the training text as the end of a line) then add a full stop and stop generating.\n",
        "\n",
        "generateLine and generatePoem will be functions allowing variable line length and line number."
      ],
      "metadata": {
        "id": "eJh-dmpvqdGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateLine(line_length):\n",
        "    random_line = []\n",
        "\n",
        "    words = list(pi.keys())\n",
        "    weights = list(pi.values())\n",
        "    random_line.append(random.choices(words,weights)[0])\n",
        "\n",
        "    for i in range(line_length-1):\n",
        "        if random_line[i] not in A:\n",
        "            random_line.append('.')\n",
        "            break\n",
        "        words = list(A[random_line[i]].keys())\n",
        "        weights = list(A[random_line[i]].values())\n",
        "        random_line.append(random.choices(words,weights)[0])\n",
        "    return \" \".join(random_line).replace(' .','.')"
      ],
      "metadata": {
        "id": "M_27Asm01-YV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generatePoem(line_length, line_number):\n",
        "    random_poem = []\n",
        "    for _ in range(line_number):\n",
        "        random_poem.append(generateLine(line_length))\n",
        "    return \"\\n\".join(random_poem)"
      ],
      "metadata": {
        "id": "4oYJF-112UXA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generatePoem(10,4))"
      ],
      "metadata": {
        "id": "COIXM4Wm2ruG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential improvements\n",
        "\n",
        "To make the model much better an improvement could be to transform the model into a second-order Markov model. In theory if all words and possibilities are considered then this increases the size of A from M^2 to M^3, however since in this case A only contains word sequences that actually occur in the training data it may be significantly reduced, although with a small set of training data it would likely be restricted by the limited number of possibilities."
      ],
      "metadata": {
        "id": "Pjf7EtOj6ffx"
      }
    }
  ]
}